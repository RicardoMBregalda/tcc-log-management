#!/usr/bin/env python3
"""
API Server OTIMIZADO - MongoDB + Fabric (H√≠brido)

OTIMIZA√á√ïES FASE 1:
1. ‚úÖ Sincroniza√ß√£o ASS√çNCRONA com Fabric (ThreadPoolExecutor)
2. ‚úÖ Cache Redis otimizado (TTL maior, invalida√ß√£o inteligente)
3. ‚úÖ √çndices MongoDB compostos

Arquitetura:
- MongoDB: Armazenamento r√°pido NoSQL
- Fabric: Blockchain para imutabilidade (sincroniza√ß√£o em background)
- Redis: Cache otimizado para consultas
"""

from flask import Flask, request, jsonify
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import json
import requests
import time
import hashlib
import uuid
from concurrent.futures import ThreadPoolExecutor
import logging
import subprocess
import sys
import os
import atexit

# Adiciona o diret√≥rio pai ao path para importar config e utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.redis_cache import get_from_cache, set_in_cache, invalidate_cache
from src.write_ahead_log import WriteAheadLog
from pymongo import MongoClient, ASCENDING, DESCENDING
from pymongo.errors import DuplicateKeyError, ConnectionFailure

# Imports do projeto
from config import (
    # MongoDB
    MONGO_URL, MONGO_DB, MONGO_COLLECTION,
    MONGO_MIN_POOL_SIZE, MONGO_MAX_POOL_SIZE,
    MONGO_MAX_IDLE_TIME_MS, MONGO_SERVER_SELECTION_TIMEOUT_MS,
    # Fabric
    FABRIC_API_URL, FABRIC_CHANNEL, FABRIC_CHAINCODE,
    FABRIC_SYNC_MAX_WORKERS,
    HTTP_POOL_CONNECTIONS, HTTP_POOL_MAXSIZE, HTTP_MAX_RETRIES,
    # Flask
    FLASK_HOST, FLASK_PORT, FLASK_DEBUG,
    # Merkle Auto-Batching
    AUTO_BATCH_ENABLED, AUTO_BATCH_SIZE, AUTO_BATCH_INTERVAL,
    BATCH_EXECUTOR_MAX_WORKERS
)
from utils import (
    print_header,
    print_section,
    get_timestamp,
    save_json,
    load_json
)

app = Flask(__name__)

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‚ú® OTIMIZA√á√ÉO 1: ThreadPool para sincroniza√ß√£o ass√≠ncrona com Fabric
fabric_executor = ThreadPoolExecutor(
    max_workers=FABRIC_SYNC_MAX_WORKERS,
    thread_name_prefix='fabric-sync'
)

# MongoDB Connection (Connection Pooling autom√°tico no PyMongo)
try:
    mongo_client = MongoClient(
        MONGO_URL,
        maxPoolSize=MONGO_MAX_POOL_SIZE,
        minPoolSize=MONGO_MIN_POOL_SIZE,
        maxIdleTimeMS=MONGO_MAX_IDLE_TIME_MS,
        serverSelectionTimeoutMS=MONGO_SERVER_SELECTION_TIMEOUT_MS
    )
    # Testa conex√£o
    mongo_client.admin.command('ping')
    db = mongo_client[MONGO_DB]
    logs_collection = db[MONGO_COLLECTION]
    sync_control_collection = db['sync_control']
    
    # ‚ú® OTIMIZA√á√ÉO 3: √çndices compostos otimizados
    logger.info("Criando √≠ndices otimizados...")
    
    # √çndices b√°sicos
    logs_collection.create_index([("id", ASCENDING)], unique=True)
    logs_collection.create_index([("timestamp", DESCENDING)])
    
    # √çndices compostos para queries comuns (NOVO)
    logs_collection.create_index([("source", ASCENDING), ("timestamp", DESCENDING)])
    logs_collection.create_index([("level", ASCENDING), ("timestamp", DESCENDING)])
    logs_collection.create_index([("source", ASCENDING), ("level", ASCENDING), ("timestamp", DESCENDING)])
    
    # √çndice para pagina√ß√£o eficiente
    logs_collection.create_index([("created_at", DESCENDING)])
    
    # √çndices para sync_control
    sync_control_collection.create_index([("log_id", ASCENDING)], unique=True)
    sync_control_collection.create_index([("sync_status", ASCENDING)])
    
    logger.info("‚úÖ MongoDB conectado com √≠ndices otimizados!")
    logger.info(f"   - Database: {MONGO_DB}")
    logger.info(f"   - Collection: {MONGO_COLLECTION}")
    logger.info(f"   - Pool: {MONGO_MIN_POOL_SIZE}-{MONGO_MAX_POOL_SIZE} conex√µes")
    logger.info(f"   - √çndices: 8 √≠ndices criados")
    
except ConnectionFailure as e:
    logger.error(f"‚ùå Erro ao conectar no MongoDB: {e}")
    exit(1)

# HTTP Session Pooling OTIMIZADO
http_session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=HTTP_POOL_CONNECTIONS,
    pool_maxsize=HTTP_POOL_MAXSIZE,
    max_retries=HTTP_MAX_RETRIES,
    pool_block=False
)
http_session.mount('http://', adapter)
http_session.mount('https://', adapter)

logger.info(f"‚úÖ HTTP Session Pool otimizado: {HTTP_POOL_CONNECTIONS} conex√µes, {HTTP_POOL_MAXSIZE} max")

# ========================================
# üîí MERKLE TREE AUTO-BATCHING CONFIG
# ========================================
# Executor para processamento de batches em background
batch_executor = ThreadPoolExecutor(
    max_workers=BATCH_EXECUTOR_MAX_WORKERS,
    thread_name_prefix='batch-processor'
)

logger.info(f"‚úÖ Auto-Batching Merkle Tree configurado:")
logger.info(f"   - Ativado: {AUTO_BATCH_ENABLED}")
logger.info(f"   - Tamanho do batch: {AUTO_BATCH_SIZE} logs")
logger.info(f"   - Intervalo: {AUTO_BATCH_INTERVAL}s")


# ========================================
# üîí WRITE-AHEAD LOG (WAL) INITIALIZATION
# ========================================

# Fun√ß√£o para inserir no MongoDB de forma segura (callback para WAL)
def insert_to_mongodb_safe(log_doc: Dict[str, Any]) -> bool:
    """
    Insere log no MongoDB de forma segura.
    Usado como callback pelo WAL processor.
    
    Args:
        log_doc: Documento do log (created_at j√° est√° em formato ISO string)
        
    Returns:
        True se inser√ß√£o bem-sucedida, False caso contr√°rio
    """
    try:
        logs_collection.insert_one(log_doc)
        logger.info(f"‚úÖ WAL processou log: {log_doc.get('id', 'unknown')}")
        return True
    except DuplicateKeyError:
        # Log j√° existe (pode ter sido inserido diretamente antes)
        logger.warning(f"‚ö†Ô∏è WAL: Log duplicado ignorado: {log_doc.get('id', 'unknown')}")
        return True  # Considera sucesso pois log j√° est√° no banco
    except Exception as e:
        logger.error(f"‚ùå WAL: Erro ao inserir log: {e}")
        return False

# Inicializar WAL
WAL_DIR = '/var/log/tcc-wal'
os.makedirs(WAL_DIR, exist_ok=True)

logger.info(f"üîí Inicializando Write-Ahead Log...")
WAL = WriteAheadLog(
    wal_dir=WAL_DIR,
    check_interval=5  # Verifica logs pendentes a cada 5 segundos
)

# Iniciar processor em background
WAL.start_processor(insert_to_mongodb_safe)

pending = WAL.get_stats()['pending_count']
if pending > 0:
    logger.warning(f"‚ö†Ô∏è WAL recuperou {pending} logs pendentes para processar")
else:
    logger.info(f"‚úÖ WAL iniciado com 0 logs pendentes")

# Handler para shutdown gracioso
def shutdown_wal():
    """Para o WAL processor ao encerrar a aplica√ß√£o"""
    logger.info("üõë Parando WAL processor...")
    WAL.stop_processor()
    logger.info("‚úÖ WAL processor parado")

atexit.register(shutdown_wal)

logger.info(f"‚úÖ Write-Ahead Log configurado:")
logger.info(f"   - Diret√≥rio: {WAL_DIR}")
logger.info(f"   - Intervalo de verifica√ß√£o: 5s")
logger.info(f"   - Garantia: 0% de perda de dados")


# ========================================
# FABRIC DIRECT QUERY HELPER
# ========================================

def query_chaincode_direct(function_name: str, args: List[str]) -> Optional[Dict[str, Any]]:
    """
    Consulta chaincode diretamente via docker exec (workaround para gateway ausente)
    
    Args:
        function_name: Nome da fun√ß√£o do chaincode
        args: Lista de argumentos
        
    Returns:
        Resposta do chaincode ou None em caso de erro
    """
    try:
        # Monta o JSON de args
        args_json = json.dumps({"function": function_name, "Args": args})
        
        # Executa query via docker usando constantes do config
        cmd = [
            "docker", "exec", "cli",
            "peer", "chaincode", "query",
            "-C", FABRIC_CHANNEL,
            "-n", FABRIC_CHAINCODE,
            "-c", args_json
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0 and result.stdout.strip():
            # Tenta fazer parse do JSON
            try:
                return json.loads(result.stdout.strip())
            except json.JSONDecodeError:
                # Se n√£o for JSON, retorna como string
                return {"result": result.stdout.strip()}
        else:
            logger.error(f"Chaincode query failed: {result.stderr}")
            return None
            
    except Exception as e:
        logger.error(f"Error querying chaincode: {e}")
        return None


def invoke_chaincode_direct(function_name, args):
    """
    Invoca o chaincode diretamente via docker exec usando peer chaincode invoke.
    
    Args:
        function_name: Nome da fun√ß√£o do chaincode
        args: Lista de argumentos
        
    Returns:
        bool: True se a invoca√ß√£o foi bem-sucedida, False caso contr√°rio
    """
    try:
        # Monta o JSON de args
        args_json = json.dumps({"function": function_name, "Args": args})
        
        # Executa invoke via docker
        cmd = [
            "docker", "exec", "cli",
            "peer", "chaincode", "invoke",
            "-o", "orderer.example.com:7050",
            "--tls",
            "--cafile", "/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem",
            "-C", "logchannel",
            "-n", "logchaincode",
            "-c", args_json
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            logger.info(f"Chaincode invoke successful: {function_name}")
            return True
        else:
            logger.error(f"Chaincode invoke failed: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"Error invoking chaincode: {e}")
        return False


# ========================================
# MERKLE TREE FUNCTIONS
# ========================================

def calculate_log_hash(log_data):
    """
    Calcula hash SHA256 de um log individual
    
    Args:
        log_data: Dicion√°rio com dados do log
        
    Returns:
        str: Hash hexadecimal do log
    """
    content = f"{log_data['id']}{log_data['timestamp']}{log_data['source']}{log_data['level']}{log_data['message']}"
    if 'metadata' in log_data:
        content += json.dumps(log_data['metadata'], sort_keys=True)
    if 'stacktrace' in log_data:
        content += log_data['stacktrace']
    return hashlib.sha256(content.encode()).hexdigest()


def combine_hashes(hash1, hash2):
    """
    Combina dois hashes usando SHA256
    
    Args:
        hash1: Primeiro hash (hex string)
        hash2: Segundo hash (hex string)
        
    Returns:
        str: Hash combinado (hex string)
    """
    combined = hash1 + hash2
    return hashlib.sha256(combined.encode()).hexdigest()


def build_merkle_tree(hashes):
    """
    Constr√≥i uma Merkle Tree a partir de uma lista de hashes
    
    Args:
        hashes: Lista de hashes (hex strings)
        
    Returns:
        str: Merkle Root (hex string)
    """
    if not hashes:
        return ""
    
    if len(hashes) == 1:
        return hashes[0]
    
    # Copia os hashes para n√£o modificar a lista original
    current_level = hashes.copy()
    
    # Constr√≥i a √°rvore bottom-up
    while len(current_level) > 1:
        next_level = []
        
        # Se o n√∫mero de n√≥s for √≠mpar, duplica o √∫ltimo
        if len(current_level) % 2 != 0:
            current_level.append(current_level[-1])
        
        # Combina pares de hashes
        for i in range(0, len(current_level), 2):
            combined = combine_hashes(current_level[i], current_level[i + 1])
            next_level.append(combined)
        
        current_level = next_level
    
    return current_level[0]


def calculate_merkle_root(logs):
    """
    Calcula a Merkle Root de um lote de logs
    
    Args:
        logs: Lista de dicion√°rios de logs
        
    Returns:
        tuple: (merkle_root, list_of_hashes)
    """
    hashes = [calculate_log_hash(log) for log in logs]
    merkle_root = build_merkle_tree(hashes)
    return merkle_root, hashes


def store_merkle_batch(batch_id, logs, merkle_root):
    """
    Armazena um batch de logs no Fabric com Merkle Root
    
    Args:
        batch_id: ID √∫nico do batch
        logs: Lista de logs do batch
        merkle_root: Raiz de Merkle calculada
        
    Returns:
        bool: True se sucesso, False caso contr√°rio
    """
    try:
        timestamp = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
        log_ids = [log['id'] for log in logs]
        
        # Invoca chaincode diretamente via docker exec
        success = invoke_chaincode_direct('StoreMerkleRoot', [
            batch_id,
            merkle_root,
            timestamp,
            str(len(logs)),
            json.dumps(log_ids)
        ])
        
        if success:
            logger.info(f"‚úÖ Merkle batch {batch_id} stored in Fabric (Root: {merkle_root[:16]}...)")
            return True
        else:
            logger.error(f"‚ùå Failed to store Merkle batch {batch_id}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error storing Merkle batch {batch_id}: {e}")
        return False


def process_pending_batch():
    """
    üîí Processa logs pendentes criando batch com Merkle Tree
    
    Esta fun√ß√£o:
    1. Busca logs pendentes de batching
    2. Calcula Merkle Root
    3. Armazena batch no Fabric
    4. Atualiza status dos logs
    """
    try:
        # Busca logs pendentes (limitado ao tamanho do batch)
        pending_logs = list(sync_control_collection.find(
            {'sync_status': 'pending_batch'},
            sort=[('created_at', ASCENDING)],
            limit=AUTO_BATCH_SIZE
        ))
        
        if not pending_logs or len(pending_logs) < 1:
            logger.debug("Nenhum log pendente para batching")
            return False
        
        # Extrai IDs dos logs
        log_ids = [log['log_id'] for log in pending_logs]
        
        # Busca dados completos dos logs no MongoDB
        logs = list(logs_collection.find(
            {'id': {'$in': log_ids}},
            {'_id': 0}
        ).sort('created_at', ASCENDING))
        
        if not logs:
            logger.warning("Logs pendentes n√£o encontrados no MongoDB")
            return False
        
        # Gera batch ID
        batch_id = f"batch_{datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')}"
        
        # Calcula Merkle Root
        merkle_root, hashes = calculate_merkle_root(logs)
        
        logger.info(f"üì¶ Processando batch {batch_id} com {len(logs)} logs...")
        
        # Armazena batch no Fabric
        if store_merkle_batch(batch_id, logs, merkle_root):
            # Atualiza logs no MongoDB com batch_id
            logs_collection.update_many(
                {'id': {'$in': log_ids}},
                {'$set': {
                    'batch_id': batch_id,
                    'merkle_root': merkle_root,
                    'batched_at': datetime.utcnow()
                }}
            )
            
            # Atualiza status de sincroniza√ß√£o para 'synced'
            sync_control_collection.update_many(
                {'log_id': {'$in': log_ids}},
                {'$set': {
                    'sync_status': 'synced',
                    'batch_id': batch_id,
                    'synced_at': datetime.utcnow()
                }}
            )
            
            logger.info(f"‚úÖ Batch {batch_id} processado com sucesso! Merkle Root: {merkle_root[:16]}...")
            return True
        else:
            logger.error(f"‚ùå Falha ao armazenar batch {batch_id}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao processar batch: {e}")
        return False


def store_merkle_batch(batch_id, logs, merkle_root):
    """
    Armazena um batch de logs no Fabric com Merkle Root
    
    Args:
        batch_id: ID √∫nico do batch
        logs: Lista de logs do batch
        merkle_root: Raiz de Merkle calculada
        
    Returns:
        bool: True se sucesso, False caso contr√°rio
    """
    try:
        timestamp = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
        log_ids = [log['id'] for log in logs]
        
        # Invoca chaincode diretamente via docker exec
        success = invoke_chaincode_direct('StoreMerkleRoot', [
            batch_id,
            merkle_root,
            timestamp,
            str(len(logs)),
            json.dumps(log_ids)
        ])
        
        if success:
            logger.info(f"‚úÖ Merkle batch {batch_id} stored in Fabric (Root: {merkle_root[:16]}...)")
            return True
        else:
            logger.error(f"‚ùå Failed to store Merkle batch {batch_id}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error storing Merkle batch {batch_id}: {e}")
        return False


# ========================================
# FABRIC SYNC FUNCTIONS
# ========================================


def send_to_fabric_async(log_data, log_id):
    """
    ‚ú® MODIFICADO: Marca log como pendente para batching (n√£o envia individualmente)
    
    Com AUTO_BATCH_ENABLED, logs n√£o s√£o enviados individualmente para o Fabric.
    Em vez disso, s√£o agrupados em batches com Merkle Tree.
    """
    try:
        if not AUTO_BATCH_ENABLED:
            # Modo legado: envia log individual (sem Merkle Tree)
            success = invoke_chaincode_direct('CreateLog', [
                log_data['id'],
                log_data['hash'],
                log_data['timestamp'],
                log_data['source'],
                log_data['level'],
                log_data['message'],
                json.dumps(log_data.get('metadata', {})),
                log_data.get('stacktrace', '')
            ])

            if success:
                sync_control_collection.update_one(
                    {'log_id': log_id},
                    {'$set': {
                        'sync_status': 'synced',
                        'fabric_tx_id': 'invoke_direct',
                        'synced_at': datetime.utcnow()
                    }},
                    upsert=True
                )
                logger.info(f"‚úÖ Log {log_id} sincronizado com Fabric")
                return True
            else:
                raise Exception("Fabric invoke failed")
        else:
            # Modo Merkle Tree: marca como pendente para batching
            sync_control_collection.update_one(
                {'log_id': log_id},
                {'$set': {
                    'sync_status': 'pending_batch',
                    'created_at': datetime.utcnow()
                }},
                upsert=True
            )
            logger.debug(f"üì¶ Log {log_id} marcado para batching")
            
            # Verifica se j√° temos logs suficientes para criar um batch
            pending_count = sync_control_collection.count_documents({'sync_status': 'pending_batch'})
            if pending_count >= AUTO_BATCH_SIZE:
                # Agenda processamento de batch em background
                batch_executor.submit(process_pending_batch)
            
            return True

    except Exception as e:
        # Atualiza status de sincroniza√ß√£o como FALHA
        sync_control_collection.update_one(
            {'log_id': log_id},
            {'$set': {
                'sync_status': 'failed',
                'error': str(e),
                'failed_at': datetime.utcnow()
            }},
            upsert=True
        )
        logger.warning(f"‚ö†Ô∏è  Falha ao sincronizar log {log_id} com Fabric: {e}")
        return False


@app.route('/health', methods=['GET'])
def health() -> Tuple[Dict[str, Any], int]:
    """Health check endpoint with WAL status"""
    wal_stats = WAL.get_stats()
    
    return jsonify({
        'status': 'healthy',
        'database': 'mongodb',
        'blockchain': 'fabric',
        'wal': {
            'enabled': True,
            'pending_count': wal_stats['pending_count'],
            'total_written': wal_stats['total_written'],
            'total_processed': wal_stats['total_processed'],
            'processor_running': wal_stats['processor_running'],
            'guarantee': '0% data loss'
        },
        'optimizations': {
            'async_fabric_sync': True,
            'redis_cache_optimized': True,
            'mongodb_compound_indexes': True,
            'connection_pool_size': f'{MONGO_MIN_POOL_SIZE}-{MONGO_MAX_POOL_SIZE}',
            'write_ahead_log': True
        }
    }), 200


@app.route('/wal/stats', methods=['GET'])
def wal_stats() -> Tuple[Dict[str, Any], int]:
    """
    Retorna estat√≠sticas do Write-Ahead Log
    
    √ötil para monitoramento e debugging
    """
    stats = WAL.get_stats()
    
    return jsonify({
        'wal_statistics': stats,
        'status': 'healthy' if stats['processor_running'] else 'processor_stopped',
        'data_loss_guarantee': '0%' if stats['processor_running'] else 'at_risk'
    }), 200


@app.route('/wal/force-process', methods=['POST'])
def wal_force_process() -> Tuple[Dict[str, Any], int]:
    """
    For√ßa processamento imediato de logs pendentes no WAL
    
    √ötil para testes e recovery manual
    """
    try:
        # For√ßa uma itera√ß√£o do processor
        WAL._process_pending_logs()
        
        stats = WAL.get_stats()
        
        return jsonify({
            'status': 'success',
            'message': 'Forced WAL processing completed',
            'pending_count': stats['pending_count'],
            'total_processed': stats['total_processed']
        }), 200
        
    except Exception as e:
        logger.error(f"Error forcing WAL processing: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500


@app.route('/logs', methods=['POST'])
def create_log() -> Tuple[Dict[str, Any], int]:
    """
    ‚ú® OTIMIZADO: Cria novo log com WAL + sincroniza√ß√£o ASS√çNCRONA
    
    Fluxo:
    1. üîí Escreve no WAL (garantia de durabilidade em disco)
    2. Tenta inserir no MongoDB imediatamente (best effort)
    3. Se MongoDB falhar, WAL reprocessa automaticamente em background
    4. Agenda sincroniza√ß√£o com Fabric em background
    5. Retorna imediatamente com garantia de 0% de perda
    
    Resultado: Lat√™ncia m√≠nima + 0% de perda de dados!
    """
    data = request.json

    # Valida√ß√£o
    required_fields = ['source', 'level', 'message']
    if not all(field in data for field in required_fields):
        return jsonify({'error': 'Missing required fields'}), 400

    # Usa ID fornecido pelo cliente, ou gera novo UUID4 se n√£o fornecido
    log_id = data.get('id', f"{data['source']}_{uuid.uuid4().hex[:16]}")
    timestamp = data.get('timestamp', get_timestamp())
    
    # Gera hash do conte√∫do para integridade
    content = f"{log_id}{timestamp}{data['source']}{data['level']}{data['message']}"
    log_hash = hashlib.sha256(content.encode()).hexdigest()

    # Documento MongoDB (created_at como ISO string para compatibilidade com WAL JSON)
    created_at_str = datetime.utcnow().isoformat()
    log_doc = {
        'id': log_id,
        'hash': log_hash,
        'timestamp': timestamp,
        'source': data['source'],
        'level': data['level'],
        'message': data['message'],
        'metadata': data.get('metadata', {}),
        'created_at': created_at_str
    }
    
    # Add stacktrace if provided (optional field)
    if 'stacktrace' in data and data['stacktrace']:
        log_doc['stacktrace'] = data['stacktrace']

    try:
        # üîí PASSO 1: Escrever no WAL PRIMEIRO (garantia de durabilidade)
        wal_success = WAL.write(log_doc)
        if not wal_success:
            # Falha cr√≠tica ao escrever no WAL
            logger.error(f"‚ùå CR√çTICO: Falha ao escrever no WAL: {log_id}")
            return jsonify({
                'error': 'Failed to write to Write-Ahead Log',
                'durability': 'NOT_GUARANTEED'
            }), 500

        # Log est√° seguro no disco (WAL) - podemos garantir sucesso
        mongodb_status = 'pending_in_wal'
        
        # PASSO 2: Tentar inserir no MongoDB imediatamente (best effort)
        try:
            logs_collection.insert_one(log_doc)
            mongodb_status = 'inserted'
            logger.info(f"‚úÖ Log inserido direto no MongoDB: {log_id}")
            
            # ‚ú® OTIMIZA√á√ÉO 1: Agenda sincroniza√ß√£o ASS√çNCRONA com Fabric
            log_data = {
                'id': log_id,
                'hash': log_hash,
                'timestamp': timestamp,
                'source': data['source'],
                'level': data['level'],
                'message': data['message'],
                'metadata': data.get('metadata', {})
            }
            
            # Include stacktrace in Fabric sync if present
            if 'stacktrace' in data and data['stacktrace']:
                log_data['stacktrace'] = data['stacktrace']
            
            # Inicia sync em background (n√£o bloqueia)
            fabric_executor.submit(send_to_fabric_async, log_data, log_id)
            
            # Cria registro de sincroniza√ß√£o como PENDENTE
            sync_control_collection.insert_one({
                'log_id': log_id,
                'sync_status': 'pending',
                'created_at': datetime.utcnow()
            })
            
            # ‚ú® OTIMIZA√á√ÉO 2: Invalida√ß√£o inteligente de cache
            invalidate_cache(f'logs_list_{data["source"]}*')
            invalidate_cache(f'logs_list_None*')
            
        except DuplicateKeyError:
            # Log j√° existe (pode ter sido processado pelo WAL antes)
            mongodb_status = 'duplicate_ignored'
            logger.warning(f"‚ö†Ô∏è Log duplicado ignorado: {log_id}")
            
        except Exception as e:
            # MongoDB falhou - WAL vai reprocessar em background
            mongodb_status = 'pending_in_wal'
            logger.warning(f"‚ö†Ô∏è MongoDB falhou, WAL vai reprocessar: {log_id} - {e}")

        # Retorna IMEDIATAMENTE com garantia de durabilidade
        return jsonify({
            'id': log_id,
            'hash': log_hash,
            'status': 'success',
            'mongodb_status': mongodb_status,
            'fabric_sync': 'pending',
            'durability': 'GUARANTEED_BY_WAL',
            'message': 'Log persisted to WAL with 0% loss guarantee'
        }), 201

    except Exception as e:
        # Erro inesperado
        logger.error(f"‚ùå Erro inesperado ao criar log: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/logs', methods=['GET'])
def get_logs() -> Tuple[Dict[str, Any], int]:
    """
    ‚ú® OTIMIZADO: Lista logs com cache inteligente
    
    Melhorias:
    - TTL de cache aumentado para 10 minutos
    - √çndices compostos para queries mais r√°pidas
    - Proje√ß√£o otimizada
    """
    # Par√¢metros
    source = request.args.get('source')
    level = request.args.get('level')
    limit = int(request.args.get('limit', 100))
    offset = int(request.args.get('offset', 0))

    # ‚ú® OTIMIZA√á√ÉO 2: Chave de cache otimizada
    cache_key = f"logs_list_{source}_{level}_{limit}_{offset}"
    
    # Tenta cache
    cached = get_from_cache(cache_key)
    if cached:
        return jsonify({
            'logs': cached,
            'cached': True,
            'count': len(cached)
        }), 200

    try:
        # Query MongoDB com √≠ndices compostos
        query = {}
        if source:
            query['source'] = source
        if level:
            query['level'] = level

        # Busca com proje√ß√£o otimizada
        logs = list(logs_collection.find(
            query,
            {'_id': 0}  # Exclui _id do MongoDB
        ).sort('timestamp', -1).skip(offset).limit(limit))

        # Converte datetime para string
        for log in logs:
            if isinstance(log.get('created_at'), datetime):
                log['created_at'] = log['created_at'].isoformat()

        # ‚ú® OTIMIZA√á√ÉO 2: TTL aumentado para 10 minutos (600s)
        set_in_cache(cache_key, logs, ttl=600)

        return jsonify({
            'logs': logs,
            'cached': False,
            'count': len(logs)
        }), 200

    except Exception as e:
        logger.error(f"Error getting logs: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/logs/<log_id>', methods=['GET'])
def get_log_by_id(log_id: str) -> Tuple[Dict[str, Any], int]:
    """
    ‚ú® OTIMIZADO: Busca log espec√≠fico com cache de 15 minutos
    """
    # Tenta cache primeiro
    cache_key = f"log_{log_id}"
    cached = get_from_cache(cache_key)
    if cached:
        return jsonify(cached), 200

    try:
        log = logs_collection.find_one({'id': log_id}, {'_id': 0})
        
        if not log:
            return jsonify({'error': 'Log not found'}), 404
        
        # Converte datetime
        if isinstance(log.get('created_at'), datetime):
            log['created_at'] = log['created_at'].isoformat()
        
        # Busca status de sincroniza√ß√£o
        sync_status = sync_control_collection.find_one(
            {'log_id': log_id},
            {'_id': 0}
        )
        
        if sync_status:
            log['fabric_sync'] = sync_status
        
        # Cache por 15 minutos (logs individuais mudam raramente)
        set_in_cache(cache_key, log, ttl=900)
        
        return jsonify(log), 200
        
    except Exception as e:
        logger.error(f"Error getting log {log_id}: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/merkle/batch', methods=['POST'])
def create_merkle_batch() -> Tuple[Dict[str, Any], int]:
    """
    Cria um batch de logs com Merkle Root
    
    Body:
    {
        "batch_size": 100  (opcional, padr√£o: 100)
    }
    
    Pega os √∫ltimos N logs sem batch_id, calcula Merkle Root e armazena no Fabric
    """
    try:
        batch_size = request.json.get('batch_size', 100) if request.json else 100
        
        # Busca logs sem batch_id
        logs = list(logs_collection.find(
            {'batch_id': {'$exists': False}},
            sort=[('created_at', ASCENDING)],
            limit=batch_size
        ))
        
        if not logs:
            return jsonify({'message': 'No logs available for batching'}), 200
        
        # Gera batch ID com get_timestamp() de utils
        batch_id = f"batch_{get_timestamp().replace(':', '').replace('-', '_')}"
        
        # Calcula Merkle Root
        merkle_root, hashes = calculate_merkle_root(logs)
        
        # Armazena batch no Fabric
        if store_merkle_batch(batch_id, logs, merkle_root):
            # Atualiza logs no MongoDB com batch_id
            log_ids = [log['id'] for log in logs]
            logs_collection.update_many(
                {'id': {'$in': log_ids}},
                {'$set': {
                    'batch_id': batch_id,
                    'merkle_root': merkle_root,
                    'batched_at': datetime.utcnow()
                }}
            )
            
            return jsonify({
                'batch_id': batch_id,
                'merkle_root': merkle_root,
                'num_logs': len(logs),
                'log_ids': log_ids,
                'status': 'success',
                'message': 'Merkle batch created and stored in blockchain'
            }), 201
        else:
            return jsonify({'error': 'Failed to store batch in Fabric'}), 500
            
    except Exception as e:
        logger.error(f"Error creating Merkle batch: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/merkle/batch/<batch_id>', methods=['GET'])
def get_merkle_batch(batch_id: str) -> Tuple[Dict[str, Any], int]:
    """
    Retorna informa√ß√µes de um batch de Merkle
    """
    try:
        # Busca batch no Fabric usando query direta
        batch = query_chaincode_direct('QueryMerkleBatch', [batch_id])
        
        if batch:
            # Busca logs do batch no MongoDB
            logs = list(logs_collection.find(
                {'batch_id': batch_id},
                {'_id': 0}
            ))
            
            return jsonify({
                'batch': batch,
                'logs': logs,
                'num_logs': len(logs)
            }), 200
        else:
            return jsonify({'error': 'Batch not found in blockchain'}), 404
            
    except Exception as e:
        logger.error(f"Error getting Merkle batch {batch_id}: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/merkle/verify/<batch_id>', methods=['POST'])
def verify_merkle_batch(batch_id: str) -> Tuple[Dict[str, Any], int]:
    """
    Verifica a integridade de um batch recalculando Merkle Root
    """
    try:
        # Busca logs do batch no MongoDB NA MESMA ORDEM da cria√ß√£o
        logs = list(logs_collection.find(
            {'batch_id': batch_id},
            {'_id': 0}
        ).sort('created_at', ASCENDING))
        
        if not logs:
            return jsonify({'error': 'Batch not found in MongoDB'}), 404
        
        # Recalcula Merkle Root
        recalculated_root, hashes = calculate_merkle_root(logs)
        
        # Busca Merkle Root original do Fabric
        batch_data = query_chaincode_direct('QueryMerkleBatch', [batch_id])
        
        if batch_data:
            original_root = batch_data.get('merkle_root', '')
            
            # Compara roots
            is_valid = (original_root == recalculated_root)
            
            return jsonify({
                'batch_id': batch_id,
                'is_valid': is_valid,
                'num_logs': len(logs),
                'original_merkle_root': original_root,
                'recalculated_merkle_root': recalculated_root,
                'integrity': 'OK' if is_valid else 'COMPROMISED',
                'message': 'Batch integrity verified successfully' if is_valid else 'WARNING: Batch integrity compromised!'
            }), 200
        else:
            return jsonify({'error': 'Failed to verify batch in Fabric'}), 500
            
    except Exception as e:
        logger.error(f"Error verifying Merkle batch {batch_id}: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/merkle/batches', methods=['GET'])
def list_merkle_batches() -> Tuple[Dict[str, Any], int]:
    """
    Lista todos os batches de Merkle
    """
    try:
        # Busca todos os batches distintos no MongoDB
        batches = logs_collection.aggregate([
            {'$match': {'batch_id': {'$exists': True}}},
            {'$group': {
                '_id': '$batch_id',
                'merkle_root': {'$first': '$merkle_root'},
                'num_logs': {'$sum': 1},
                'batched_at': {'$first': '$batched_at'}
            }},
            {'$sort': {'batched_at': -1}}
        ])
        
        batch_list = []
        for batch in batches:
            batch_list.append({
                'batch_id': batch['_id'],
                'merkle_root': batch['merkle_root'],
                'num_logs': batch['num_logs'],
                'batched_at': batch['batched_at'].isoformat() if batch.get('batched_at') else None
            })
        
        return jsonify({
            'batches': batch_list,
            'total_batches': len(batch_list)
        }), 200
        
    except Exception as e:
        logger.error(f"Error listing Merkle batches: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/stats', methods=['GET'])
def get_stats() -> Tuple[Dict[str, Any], int]:
    """Estat√≠sticas do sistema"""
    try:
        total_logs = logs_collection.count_documents({})
        
        sync_stats = list(sync_control_collection.aggregate([
            {'$group': {
                '_id': '$sync_status',
                'count': {'$sum': 1}
            }}
        ]))
        
        sync_summary = {stat['_id']: stat['count'] for stat in sync_stats}
        
        return jsonify({
            'total_logs': total_logs,
            'fabric_sync_status': sync_summary,
            'optimizations_active': True
        }), 200
        
    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        return jsonify({'error': str(e)}), 500


# ========================================
# üîí SCHEDULER PARA AUTO-BATCHING
# ========================================
import threading

def schedule_batch_processing():
    """
    Agenda processamento peri√≥dico de batches
    Executa a cada AUTO_BATCH_INTERVAL segundos
    """
    if AUTO_BATCH_ENABLED:
        batch_executor.submit(process_pending_batch)
        # Reagenda para pr√≥xima execu√ß√£o
        timer = threading.Timer(AUTO_BATCH_INTERVAL, schedule_batch_processing)
        timer.daemon = True
        timer.start()


if __name__ == '__main__':
    print("\n" + "="*60)
    print("üöÄ API Server OTIMIZADO - MongoDB + Fabric H√≠brido")
    print("="*60)
    print("üìä OTIMIZA√á√ïES ATIVAS:")
    print("  ‚úÖ 1. Sincroniza√ß√£o ASS√çNCRONA com Fabric (-80% lat√™ncia)")
    print("  ‚úÖ 2. Cache Redis otimizado (TTL 10-15min)")
    print("  ‚úÖ 3. √çndices MongoDB compostos (+30% queries)")
    print(f"  ‚úÖ 4. Connection Pool {MONGO_MIN_POOL_SIZE}-{MONGO_MAX_POOL_SIZE} conex√µes")
    
    if AUTO_BATCH_ENABLED:
        print("  ‚úÖ 5. üîí Auto-Batching Merkle Tree ATIVADO")
        print(f"     - Tamanho do batch: {AUTO_BATCH_SIZE} logs")
        print(f"     - Intervalo: {AUTO_BATCH_INTERVAL}s")
        print("     - Todos os logs passam por Merkle Tree antes do blockchain")
    
    print("="*60)
    print()
    
    # Inicia scheduler de auto-batching
    if AUTO_BATCH_ENABLED:
        logger.info("üîí Iniciando scheduler de auto-batching...")
        schedule_batch_processing()
    
    app.run(host=FLASK_HOST, port=FLASK_PORT, debug=FLASK_DEBUG, threaded=True)
